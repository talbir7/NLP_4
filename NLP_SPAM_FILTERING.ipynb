{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done by :\n",
    "\n",
    "    Rayhane Talbi\n",
    "    \n",
    "                                 *** Spam Filtering ***\n",
    " \n",
    "                                                                                                                    PYTHON 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBRAIRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SMSSpamCollection', header=None, sep='\\t', names=['y', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y                                                  x\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x\n",
       "y         \n",
       "ham   4825\n",
       "spam   747"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('y').count() #Counting occurencies of hams and spams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(df_):\n",
    "    df_ = df_.replace(to_replace = '', value = np.NaN) \n",
    "\n",
    "    #Delete line with nan (to be sure)\n",
    "    df1 = df_.dropna(how='any')\n",
    "\n",
    "    #Supprimer les colonnes n'ayant qu'une valeur unique\n",
    "    #df1 = df1.loc[:, (df1 != df1.iloc[0]).any()] \n",
    "\n",
    "    #Delete column with only distinct value\n",
    "    \"\"\"\n",
    "    for col in df1.columns:\n",
    "        if(pd.Series(df1[col]).dtype == \"object\"):\n",
    "            if len(df1[col].unique()) == len(df1):\n",
    "                df1 = df1.drop(col, axis=1) \n",
    "    \"\"\"\n",
    "    #Delete lines in double\n",
    "    df2=df1.drop_duplicates(keep='first')\n",
    "    df2 = df2.reset_index()\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(0\\w{3} \\w{3} \\w{4})|(0\\w{10})|(0\\w{3} \\w{3} \\w{2} \\w{2})','phonenumbr', text)\n",
    "    text = text.replace(r'£|\\$', 'moneysymb')  \n",
    "    text = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', text)\n",
    "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',text)\n",
    "    \n",
    "    text = re.sub(r'\\d+','',text)\n",
    "    text = (re.compile('[^a-z ]')).sub('',text)\n",
    "    text = text.strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    result = [token for token in tokens if not token in stop_words]\n",
    "    result = [lemmatizer.lemmatize(word) for word in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle data\n",
    "def shuffle_data(df):\n",
    "    return df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df.copy()\n",
    "cdf['x'] = cdf['x'].apply(lambda i: preprocess(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>ham</td>\n",
       "      <td>[webpage, available]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>ham</td>\n",
       "      <td>[dahe, stupid, daalways, sending, like, thisdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>ham</td>\n",
       "      <td>[shall, book, chez, jules, half, eight, thats,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yetunde, im, sorry, moji, seem, busy, able, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>ham</td>\n",
       "      <td>[sun, cant, come, earth, send, luv, ray, cloud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>ham</td>\n",
       "      <td>[taste, fish, curry, p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>ham</td>\n",
       "      <td>[knowhe, watching, film, computer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>ham</td>\n",
       "      <td>[tessypls, favor, pls, convey, birthday, wish,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>ham</td>\n",
       "      <td>[gud, gudk, chikku, tke, care, sleep, well, gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, want, make, happy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y                                                  x\n",
       "3057  ham                               [webpage, available]\n",
       "4211  ham  [dahe, stupid, daalways, sending, like, thisdo...\n",
       "1581  ham  [shall, book, chez, jules, half, eight, thats,...\n",
       "1364  ham  [yetunde, im, sorry, moji, seem, busy, able, g...\n",
       "657   ham  [sun, cant, come, earth, send, luv, ray, cloud...\n",
       "...   ...                                                ...\n",
       "1704  ham                            [taste, fish, curry, p]\n",
       "2998  ham                 [knowhe, watching, film, computer]\n",
       "2128  ham  [tessypls, favor, pls, convey, birthday, wish,...\n",
       "5240  ham  [gud, gudk, chikku, tke, care, sleep, well, gu...\n",
       "1629  ham                 [yes, princess, want, make, happy]\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf = shuffle_data(cdf)\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_validationRatio7_3(df):\n",
    "    train = df.sample(frac=0.8, random_state=200)\n",
    "    test = df.drop(train.index)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= holdout_validationRatio7_3(sf)\n",
    "tr = train.reset_index(drop=True)\n",
    "te = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVES BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculFrequence(df_):\n",
    "    df_ = df_.reset_index(drop=True)\n",
    "    freq_ ={}\n",
    "    cpt =0\n",
    "    for i in df_['x']:\n",
    "        c = Counter(i)\n",
    "        for key in c:\n",
    "            if key in freq_.keys(): \n",
    "                if df_['y'][cpt] == \"ham\":\n",
    "                    freq_[key][1] += c[key]\n",
    "                else:\n",
    "                    freq_[key][0] += c[key]    \n",
    "            else:\n",
    "                freq_[key]= list()\n",
    "                if df_['y'][cpt] == \"ham\":\n",
    "                    freq_[key].append(0) #Fist element is the number of spam\n",
    "                    freq_[key].append(c[key]) #Second is the number of ham\n",
    "                else:\n",
    "                    freq_[key].append(c[key])\n",
    "                    freq_[key].append(0)\n",
    "        cpt+=1\n",
    "    return freq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = calculFrequence(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability to be a spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pspam = tr['y'].value_counts()['spam'] / tr.shape[0]\n",
    "Pham = tr['y'].value_counts()['ham'] / tr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumberTotalWords = [sum(value) for value in zip(*freq.values())]\n",
    "\n",
    "NbWordsSpam = NumberTotalWords[0]\n",
    "\n",
    "NbWordsHam = NumberTotalWords[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAPLACE SMOOTHING\n",
    "V = 1*len(freq) #Number of total distinct words\n",
    "def p_w_spam(word, freq):\n",
    "    if word in freq.keys():\n",
    "        return (freq[word][0] +1)/(NbWordsSpam + V)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def p_w_ham(word, freq):\n",
    "    if word in freq.keys():\n",
    "        return (freq[word][1] +1)/(NbWordsHam + V)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_w_spam(word, freq)\n",
    "        p_ham_given_message *= p_w_ham(word, freq)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "te['predicted'] = te['x'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(TP, FP, FN, TN, te):\n",
    "    \"\"\"\n",
    "    This function print infomations about our predictions\n",
    "    \"\"\"\n",
    "    accuracy = (te['predicted'] == te['y']).sum() / te.shape[0] * 100\n",
    "    recall = TP / (TP + FN) *100\n",
    "    precision = TP / (TP + FP) *100\n",
    "    print(\"\\nConfusion Matrix\")\n",
    "    print(\"\\t\\t    Predict cluster1 \\t Predict cluster2\")\n",
    "    print(\"Predict cluster1 | \", TP,\"\\t\\t\\t\", FP)\n",
    "    print(\"Predict cluster2 | \", FN,\"\\t\\t\\t\", TN)\n",
    "    print(\"\\n\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision (Successfully predict cluster1): \", precision)\n",
    "    print(\"Recall (Successfully predict cluster2): \", recall)\n",
    "    print(\"F1 score : \", 2*(recall * precision)/(recall + precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    CONFUSION MATRIX\n",
    "\"\"\"\n",
    "def confusion(prediction, true_values,te):\n",
    "    \"\"\"\n",
    "    This fuctions calculte informations of the confusion matrix and \n",
    "    give a good view of the predictions using probabilities to convert\n",
    "    in integer\n",
    "    Input:\n",
    "        prediction: list of predictions\n",
    "        B: matrix of our real values 100 1\n",
    "    Output:\n",
    "        TP: integer reresenting True positive \n",
    "        FP: integer reresenting False positive\n",
    "        FN: integer reresenting False negative\n",
    "        TN: integer reresenting True negatice\n",
    "        pred: intger list of preidictions 0 or 1\n",
    "    \"\"\"\n",
    "    TP, FP, FN, TN = 0,0,0,0\n",
    "\n",
    "    for i in range(len(true_values)):\n",
    "        if true_values[i] == \"ham\" and prediction[i]== \"ham\":\n",
    "            TN += 1\n",
    "        if true_values[i] == \"spam\" and prediction[i]== \"ham\":\n",
    "            FN += 1\n",
    "        if true_values[i] == \"ham\" and prediction[i]== \"spam\":\n",
    "            FP += 1\n",
    "        if true_values[i] == \"spam\" and prediction[i]== \"spam\":\n",
    "            TP += 1\n",
    "    print_info(TP, FP, FN, TN, te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      "\t\t    Predict cluster1 \t Predict cluster2\n",
      "Predict cluster1 |  137 \t\t\t 6\n",
      "Predict cluster2 |  8 \t\t\t 963\n",
      "\n",
      "\n",
      "Accuracy:  98.74326750448833\n",
      "Precision (Successfully predict cluster1):  95.8041958041958\n",
      "Recall (Successfully predict cluster2):  94.48275862068965\n",
      "F1 score :  95.1388888888889\n"
     ]
    }
   ],
   "source": [
    "confusion(te['predicted'], te['y'],te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
